{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# или через питон\n",
    "#from utils import mnist_reader \n",
    "#X_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "#X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# через TF\n",
    "data = input_data.read_data_sets('data/fashion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-320c60bfcdf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#mnist = input_data.read_data_sets(\"/tmp/data\",one_hot=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/fashion'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py\u001b[0m in \u001b[0;36mread_data_sets\u001b[0;34m(train_dir, fake_data, one_hot, dtype, reshape, validation_size, seed)\u001b[0m\n\u001b[1;32m    265\u001b[0m   \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m   \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m   \u001b[0mvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, images, labels, fake_data, one_hot, dtype, reshape, seed)\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# Convert from [0, 255] -> [0.0, 1.0].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import math\n",
    "\n",
    "#mnist = input_data.read_data_sets(\"/tmp/data\",one_hot=False)\n",
    "mnist = input_data.read_data_sets('data/fashion')\n",
    "\n",
    "import pdb\n",
    "def create_pairs(x, digit_indices):\n",
    "    '''Positive and negative pair creation.\n",
    "    Alternates between positive and negative pairs.\n",
    "    '''\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    n = min([len(digit_indices[d]) for d in range(10)]) - 1\n",
    "    for d in range(10):\n",
    "        for i in range(n):\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[d][i+1]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            inc = random.randrange(1, 10)\n",
    "            dn = (d + inc) % 10\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            labels += [1, 0]\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "\n",
    "def mlp(input_,input_dim,output_dim,name=\"mlp\"):\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w',[input_dim,output_dim],tf.float32,tf.random_normal_initializer(mean = 0.001,stddev=0.02))\n",
    "        return tf.nn.relu(tf.matmul(input_,w))\n",
    "        \n",
    "def build_model_mlp(X_,_dropout):\n",
    "\n",
    "    model = mlpnet(X_,_dropout)\n",
    "    return model\n",
    "\n",
    "def mlpnet(image,_dropout):\n",
    "    l1 = mlp(image,784,128,name='l1')\n",
    "    l1 = tf.nn.dropout(l1,_dropout)\n",
    "    l2 = mlp(l1,128,128,name='l2')\n",
    "    l2 = tf.nn.dropout(l2,_dropout)\n",
    "    l3 = mlp(l2,128,128,name='l3')\n",
    "    return l3\n",
    "def contrastive_loss(y,d):\n",
    "    tmp= y *tf.square(d)\n",
    "    #tmp= tf.mul(y,tf.square(d))\n",
    "    tmp2 = (1-y) *tf.square(tf.maximum((1 - d),0))\n",
    "    return tf.reduce_sum(tmp +tmp2)/batch_size/2\n",
    "\n",
    "def compute_accuracy(prediction,labels):\n",
    "    return labels[prediction.ravel() < 0.5].mean()\n",
    "    #return tf.reduce_mean(labels[prediction.ravel() < 0.5])\n",
    "def next_batch(s,e,inputs,labels):\n",
    "    input1 = inputs[s:e,0]\n",
    "    input2 = inputs[s:e,1]\n",
    "    y= np.reshape(labels[s:e],(len(range(s,e)),1))\n",
    "    return input1,input2,y\n",
    "    \n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# the data, shuffled and split between train and test sets\n",
    "X_train = mnist.train._images\n",
    "y_train = mnist.train._labels\n",
    "X_test = mnist.test._images\n",
    "y_test = mnist.test._labels\n",
    "batch_size =128\n",
    "global_step = tf.Variable(0,trainable=False)\n",
    "starter_learning_rate = 0.001\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate,global_step,10,0.1,staircase=True)\n",
    "# create training+test positive and negative pairs\n",
    "digit_indices = [np.where(y_train == i)[0] for i in range(10)]\n",
    "tr_pairs, tr_y = create_pairs(X_train, digit_indices)\n",
    "digit_indices = [np.where(y_test == i)[0] for i in range(10)]\n",
    "te_pairs, te_y = create_pairs(X_test, digit_indices)\n",
    "\n",
    "images_L = tf.placeholder(tf.float32,shape=([None,784]),name='L')\n",
    "images_R = tf.placeholder(tf.float32,shape=([None,784]),name='R')\n",
    "labels = tf.placeholder(tf.float32,shape=([None,1]),name='gt')\n",
    "dropout_f = tf.placeholder(\"float\")\n",
    "\n",
    "with tf.variable_scope(\"siamese\") as scope:\n",
    "    model1= build_model_mlp(images_L,dropout_f)\n",
    "    scope.reuse_variables()\n",
    "    model2 = build_model_mlp(images_R,dropout_f)\n",
    "\n",
    "distance  = tf.sqrt(tf.reduce_sum(tf.pow(tf.subtract(model1,model2),2),1,keep_dims=True))\n",
    "loss = contrastive_loss(labels,distance)\n",
    "#contrastice loss\n",
    "t_vars = tf.trainable_variables()\n",
    "d_vars  = [var for var in t_vars if 'l' in var.name]\n",
    "batch = tf.Variable(0)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = 0.0001).minimize(loss)\n",
    "#optimizer = tf.train.RMSPropOptimizer(0.0001,momentum=0.9,epsilon=1e-6).minimize(loss)\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(init)\n",
    "    tf.initialize_all_variables().run()\n",
    "    # Training cycle\n",
    "    for epoch in range(30):\n",
    "        avg_loss = 0.\n",
    "        avg_acc = 0.\n",
    "        total_batch = int(X_train.shape[0]/batch_size)\n",
    "        start_time = time.time()\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            s  = i * batch_size\n",
    "            e = (i+1) *batch_size\n",
    "            # Fit training using batch data\n",
    "            input1,input2,y =next_batch(s,e,tr_pairs,tr_y)\n",
    "            _,loss_value,predict=sess.run([optimizer,loss,distance], feed_dict={images_L:input1,images_R:input2 ,labels:y,dropout_f:0.9})\n",
    "            feature1=model1.eval(feed_dict={images_L:input1,dropout_f:0.9})\n",
    "            feature2=model2.eval(feed_dict={images_R:input2,dropout_f:0.9})\n",
    "            tr_acc = compute_accuracy(predict,y)\n",
    "            if math.isnan(tr_acc) and epoch != 0:\n",
    "                print('tr_acc %0.2f' % tr_acc)\n",
    "                pdb.set_trace()\n",
    "            avg_loss += loss_value\n",
    "            avg_acc +=tr_acc*100\n",
    "        #print('epoch %d loss %0.2f' %(epoch,avg_loss/total_batch))\n",
    "        duration = time.time() - start_time\n",
    "        print('epoch %d  time: %f loss %0.5f acc %0.2f' %(epoch,duration,avg_loss/(total_batch),avg_acc/total_batch))\n",
    "    y = np.reshape(tr_y,(tr_y.shape[0],1))\n",
    "    predict=distance.eval(feed_dict={images_L:tr_pairs[:,0],images_R:tr_pairs[:,1],labels:y,dropout_f:1.0})\n",
    "    tr_acc = compute_accuracy(predict,y)\n",
    "    print('Accuract training set %0.2f' % (100 * tr_acc))\n",
    "\n",
    "    # Test model\n",
    "    predict=distance.eval(feed_dict={images_L:te_pairs[:,0],images_R:te_pairs[:,1],labels:y,dropout_f:1.0})\n",
    "    y = np.reshape(te_y,(te_y.shape[0],1))\n",
    "    te_acc = compute_accuracy(predict,y)\n",
    "print('Accuract test set %0.2f' % (100 * te_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
